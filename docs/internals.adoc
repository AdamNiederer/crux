= Internals

This document discusses and suggests different internal implementation
details and design.

==== Transaction Log Format

The transactions could look something like this, the time is business
time:

[source,clj]
----
[:crux.tx/put :http://dbpedia.org/resource/Pablo_Picasso
"090622a35d4b579d2fcfebf823821298711d3867"
#inst "2018-05-18T09:20:27.966-00:00"]

[:crux.tx/cas :http://dbpedia.org/resource/Pablo_Picasso
"090622a35d4b579d2fcfebf823821298711d3867"
"048ebba27e1da223ce97dded59d46e069ddf921b"
#inst "2018-05-18T09:21:31.846-00:00"]

[:crux.tx/delete :http://dbpedia.org/resource/Pablo_Picasso
#inst "2018-05-18T09:21:52.151-00:00"]
----

In this model, upserts or merging can be implemented via CAS on the
client, first fetching the document then merging it. It could
potentially be done by Crux itself, but it complicates the ingestion, as
a new resulting document would be generated and indexed.

The key itself could also be hashed if necessary if we wish to hide it
from the transaction log, in which case you would have:

[source,clj]
----
[:crux.tx/put "9049023351ca330419c5c5072948a305343c8d91"
"090622a35d4b579d2fcfebf823821298711d3867"
#inst "2018-05-18T09:20:27.966-00:00"]
----

Where 9049023351ca330419c5c5072948a305343c8d91 is the SHA1 of the
:http://dbpedia.org/resource/Pablo_Picasso keyword.

Omitting business time defaults it to the transaction time, which is
always taken from the message itself:

[source,clj]
----
[:crux.tx/put :http://dbpedia.org/resource/Pablo_Picasso
"090622a35d4b579d2fcfebf823821298711d3867"]
----

While the business time often might stored as a field inside the entity
itself, this is not always the case and not mandatory, and the content
hash would change simply by updating a document even without any other
``real'' changes. This information is stored in the
eid/business-time/transact-time/tx-id index key so it can be accessed
regardless, and entities could optionally be enriched with this meta
data on read, see above.

== System of Record / Log

=== Proposal A: Immutable Log (Kafka)

The initial plan has been to store the transaction log in an immutable
log, such as Kafka. This is discussed in link:retention.md[retention].
In both suggestion below compaction and deletion of data becomes
cumbersome, and might be easiest use in combination with separate topics
for different retention mechanisms or if we use encrypted personal data
and forget the keys.

The messages in this log can either be individual entities, grouped into
transactions, or transactions themselves. There are some pros and cons
of both approaches:

==== Proposal Aa: Messages are Entities

Pros: + Messages can have meaningful keys. + Can potentially use
compaction. + Can potentially use Kafka transactions across topics.

Cons: + Harder to reason about transaction boundary, Kafka doesn’t
support transactional reads. + Cannot use LogAppend time in Kafka for
transaction wall time, at least not without additional logic, related to
above.

==== Proposal Ab: Messages are Transactions

Pros: + Clear transaction boundaries. + Can use LogAppend time for
transaction wall time.

Cons: + Cannot compact topics, each key (if used) is an unique
transaction id. + Business time is not necessarily the same for each
entity in the message (not necessarily a problem, but can be confusing).

==== Proposal B: Distributed CRDT KV Store

This is easier to modify and delete data from, but requires much more
engineering and is a departure from the log based design. While Kafka
might still play a role, the system of record of CRDTs must likely live
somewhere else. Either directly in the nodes KV stores - which makes the
durability guarantees of them much higher - or in another store.

As this can be a multi-master setup, it’s likely to be more scalable and
it’s also a bit more forward looking design than the idealisation of an
immutable log that has to be worked around.

== Identifiers

It’s up to the users to supply their own IDs, such as UUIDs.

The advantage of this approach are:

1.  Users get to use their own (upstream) IDs, which is more sympathetic
to the enterprise reality of multiple data-stores, and for when users
are working with external data-sets that already come with IDs.
2.  External IDs are needed anyway, if data is to be sharded across
nodes and needs to be reconciled in some way.
3.  No-need for temp IDs, thus simplicity of operation.
4.  It aligns with the intuitions of Crux being an `unbundled' DB; the
ID generation management is another piece that is unbundled, given over
to the user’s control.

The downside is that external IDs will not be optimised for internal
usage, i.e being numeric IDs to be used directly as part of Crux’s
indices. Therefore IDs may will to be mapped to/from accordingly when
data goes in and out.

This could be mitigated by using MD5s rather than numerical IDs
internally (albeit at a higher cost), but a mapping will still to be
made to reconstruct the external IDs when returning data.

== Eviction

=== Proposal A: Evict Content History of Entity

In the simplest case, we can evict the content of the history of an
entity in the log before the transaction time:

[source,clj]
----
[:crux.tx/evict :http://dbpedia.org/resource/Pablo_Picasso]
----

The indexing node will need to find all versions of the entity before
this time, and get rid of them. In Proposal A for Indexing above, the
documents must then be deleted from a compacted Kafka topic. To make the
decision one needs to have consumed the transaction log up until the
eviction message is read so one can decide which hashes to delete.

The easiest way to achieve this is to have the indexing nodes all send
deletion messages to the compacted document topic for the content hashes
from the history of an entity. This will result in duplicated (but
idempotent) messages being sent to issue deletions, but requires no new
moving piece, or relying on the client to synchronise and submit the
deletion messages to the document topic.

The indexing nodes would also listen to this topic and perform purges of
its indexes when required. It’s worth noting that apart from when
deleting a key, potentially by setting the value to nil, the key should
always be the content hash of its value.

In more advanced cases, the eviction message above could potentially
contain dates to only evict partial histories.

There’s also an issue around someone else adding a new version of an
entity at the same time when the entity is evicted, which will result in
the entity to resurface. Though this is just a special case of it
happening later, which might be totally valid. An alternative is to do a
hard eviction, that stops new versions of an entity be able to written
in the future.

=== Proposal B: Evict Content Directly

Another approach is that instead of evicting a specific entity, the user
could provide a list of content hashes to explicitly evict. Any logic or
queries could be used to build this list. This wouldn’t deal with
entities or their time lines explicitly.
