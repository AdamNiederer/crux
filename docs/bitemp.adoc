= Bitemporality

== Why?

One inherent question of using a temporal database, is how to model
time. A baseline notion of time that is always available is
'_transaction time_'; the point at which data is transacted into the
database.

_transaction time_ gives numerous benefits such as time-travelling
queries for data insight, an audit trail, and the technical basis for
an event sourced architecture.

In any situation where your database isn’t the ultimate owner of the
data and where corrections to data can flow in from various sources,
and at various times, then relying always on _transaction time_ time
does not work. We still want the ability the ability query across
time, but we want to go beyond _transaction time_. We need _business
time_.

In Crux, most queries would consider themselves with _business time_,
and _business time_ only. You will normally ask, what is the value of this
entity at _business time_, regardless if this history has been
rewritten several times from at multiple _transaction times_.

Only if you want to ensure repeatable reads or do auditing queries,
would you need to consider both _transaction time_ and _business
time_.

*In a ecosystem of many systems, where one cannot control the ultimate
time line, or other systems abilities to write into the past, one needs
bitemporality to ensure evolving but consistent views of the data.*

* https://en.wikipedia.org/wiki/Temporal_database[Temporal database]
* https://martinfowler.com/eaaDev/timeNarrative.html[Temporal Patterns]
* https://kx.com/blog/kx-insights-powering-business-decisions-bitemporal-data/[Kx Insights: Powering Business Decisions with Bitemporal Data]

== Notes:

_transaction time_ is simply _now_ for most normal queries.

Similarly, when writing data, in case there isn’t any specific
_business time_ available, _business time_ and _transaction time_ take
the same value.

== Use cases

=== Give the latest value of X at a given tx-time

For audit reasons, we might wish to know with certainty the value of a
given entity-attribute at a given tx-instant. In this case, we want to
exclude the possibility of the business past being amended, so we need a
pre-correction view of the data, relying on tx-instant.

To achieve this we will use an as-of using ts (business time) and tx-ts
(transaction time).

== Point vs Range)

One issue with writing into the past, is that it can be hard to reason
about what you actually change. If one overwrites an earlier change with
exactly the same business time, one would effectively correct this
value. But if one writes a value into the past at a business time
without a previous value, one would effectively insert a new version of
the entity into the past.

When overwriting one value in the past, the system needs to be
predictable about what happens to the next value. By default this value
wouldn’t change, but there might need to be possible to change a range
of values across business time in a consistent manner. The changes would
need to be done in a link:transactions.md[transaction].

== Event Sourcing

Writing into the past assumes that any downstream system that derives
state from the transaction log can deal with this. This is not
necessarily a safe assumption. Often writes in the past, or corrections,
are events in their own regard, which raises questions about what we
actually write into our log, raw assertions or versions of documents, or
actual events from which these are derived?

== Event Horizon

What is visible in across the graph when executing a
link:query.md[query] is a non-trivial problem. The latest version of
each entity, visible from both time lines, can vary widely. The latest
version might been written just now, or years back. Some entities might
never change, others change several times a second. Datomic’s as-of
functionality abstracts this problem away from the user for transaction
time. Our aim it to allow Crux do the same in a bitemp fashion.

As mentioned above, the transaction time is usually not used when doing
historical queries, so for most cases one only has to find the latest
version as of a specific business time. This is different from how
Datomic works, as Datomic will, by definition, always use transaction
time. When transaction time is also used for full bitemporal queries,
the result will be further limited to the latest version as of a
specific transaction time as well. Both these filters might end up
returning nothing.

The visibility of the event horizon gets further complicated in the case
of using triples, as in this case a single entity might been written and
updated over time to compose the current version by many triples, each
written at different times, and potentially also corrected.

What data that can be seen can be further limited by
link:retention.md[retention] and provenance models. Unlike corrections,
which allow bitemporal queries for repeatable reads, retention might
actually remove (or excise in Datomic) data, which will by definition
break repeatable reads, and also complicate event sourcing and other
derived views of the system.
