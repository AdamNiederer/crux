= Deployment

NOTE: TODO: Considering moving this into the user-guide

NOTE: TODO: Make an index of uris and show at the top, in a section called index

NOTE: TODO: Make this page look and feel nicer

[#rest]
== REST API

Crux offers a small REST API that allows one to use Crux in a
conventional SaaS way, deploying Kafka and query nodes into AWS and
interacting with Crux over HTTP. This mode does not support all
features.

When using the REST API, the user doesnâ€™t interact directly with
Kafka, but calls one of the query nodes (potentially behind a load
balancer) over HTTP to interact with Crux. As the query nodes might be
at different points in the index, and different queries might go to
differnet nodes, this may cause read consistency issues.

The REST API also provides an experimental endpoint for SPARQL 1.1
Protocol queries under `/sparql/`, rewriting the query into the Crux
datalog dialect. Only a small subset of SPARQL is supported, and no
other RDF features are available.

.API
[#table-conversion%header,cols="d,d,d"]
|===
|uri|method|description
|<<#home,`/`>>|GET|returns various details about the state of the database
|<<#document, `/document/[content-hash]`>>|GET or POST|returns the document for a given hash
|<<#entity, `/entity`>>|POST|Returns an entity for a given ID and optional business-time/transaction-time co-ordinates
|===

[#home]
=== GET `/`

Returns various details about the state of the database. Can be used as a health check.

[source,bash]
----
curl -X GET $nodeURL/
----
[source,clj]
----
{:crux.kv/kv-backend "crux.kv.rocksdb.RocksKv",
 :crux.kv/estimate-num-keys 92,
 :crux.kv/size 72448,
 :crux.zk/zk-active? true,
 :crux.tx-log/consumer-state
   {:crux.kafka.topic-partition/crux-docs-0
      {:offset 25,
       :time #inst "2019-01-08T11:06:41.867-00:00",
       :lag 0},
    :crux.kafka.topic-partition/crux-transaction-log-0
      {:offset 19,
       :time #inst "2019-01-08T11:06:41.869-00:00",
       :lag 0}}}
----

NOTE: `estimate-num-keys` is an (over)estimate of the number of transactions in the log (each of which is a key in RocksDB). RocksDB https://github.com/facebook/rocksdb/wiki/RocksDB-FAQ[does not provide] an exact key count.

[#document]
=== GET/POST `/document/[content-hash]`

Returns the document stored under that hash, if it exists.

[source,bash]
----
curl -X GET $nodeURL/document/7af0444315845ab3efdfbdfa516e68952c1486f2
----
[source,clj]
----
{:crux.db/id :foobar, :name "FooBar"}
----
NOTE: Hashes for older versions of a document can be obtained with `/history`, under the `:crux.db/content-hash` keys.

[#entity]
=== POST `/entity`

Takes a key and, optionally, <<bitemp.adoc#,a `:business-time` and/or `:transact-time`>> (defaulting to now). Returns the value stored under that key at those times.

[source,bash]
----
curl -X POST \
     -H "Content-Type: application/edn" \
     -d '{:eid :tommy}' \
     $nodeURL/entity
----

[source,clj]
----
{:crux.db/id :tommy, :name "Tommy", :last-name "Petrov"}
----

[source,bash]
----
curl -X POST \
     -H "Content-Type: application/edn" \
     -d '{:eid :tommy :business-time #inst "1999-01-08T14:03:27.254-00:00"}' \
     $nodeURL/entity
----

[source,clj]
----
nil
----

=== POST `/entity-tx`

Takes a key and, optionally, <<bitemp.adoc#,a `:business-time` and/or `:transact-time`>> (defaulting to now). Returns the `:put` or `:cas` transaction that most recently set that key at those times.

[source,bash]
----
curl -X POST \
     -H "Content-Type: application/edn" \
     -d '{:eid :foobar}' \
     $nodeURL/entity-tx
----
[source,clj]
----
{:crux.db/id "8843d7f92416211de9ebb963ff4ce28125932878",
 :crux.db/content-hash "7af0444315845ab3efdfbdfa516e68952c1486f2",
 :crux.db/business-time #inst "2019-01-08T16:34:47.738-00:00",
 :crux.tx/tx-id 0,
 :crux.tx/tx-time #inst "2019-01-08T16:34:47.738-00:00"}
----

`/history/[:key]`:: GET or POST: returns the transaction history of a key, from newest to oldest transaction time.
+
[source,bash]
----
curl -X GET $nodeURL/history/:ivan
----
[source,clj]
----
[{:crux.db/id "a15f8b81a160b4eebe5c84e9e3b65c87b9b2f18e",
  :crux.db/content-hash "c28f6d258397651106b7cb24bb0d3be234dc8bd1",
  :crux.db/business-time #inst "2019-01-07T14:57:08.462-00:00",
  :crux.tx/tx-id 14,
  :crux.tx/tx-time #inst "2019-01-07T16:51:55.185-00:00"}

 {...}]
----

`/query`:: POST: takes a datalog query and returns its results.
+
[source,bash]
----
curl -X POST \
     -H "Content-Type: application/edn" \
     -d '{:query {:find [e] :where [[e :last-name "Petrov"]]}}' \
     $nodeURL/query
----
[source,clj]
----
#{[:boris][:ivan]}
----

`/query-stream`:: POST: same as `/query` but the results are streamed.

`/sync`:: GET: Waits until the Kafka consumer's lag is back to 0 (i.e. when it no longer has pending transactions to write). Timeout is 10 seconds by default, but can be specified as a parameter in miliseconds. Returns the transaction time of the most recent transaction.
+
[source,bash]
----
curl -X GET $nodeURL/sync?timeout=500
----
[source,clj]
----
#inst "2019-01-08T11:06:41.869-00:00"
----

`/tx-log`:: GET: returns a list of all transactions, from oldest to newest transaction time.
+
[source,bash]
----
curl -X GET $nodeURL/tx-log
----
[source,clj]
----
({:crux.tx/tx-time #inst "2019-01-07T15:11:13.411-00:00",
  :crux.tx/tx-ops [[
    :crux.tx/put "a15f8b81a160b4eebe5c84e9e3b65c87b9b2f18e" "c28f6d258397651106b7cb24bb0d3be234dc8bd1"
    #inst "2019-01-07T14:57:08.462-00:00"]],
  :crux.tx/tx-id 0}

 {:crux.tx/tx-time #inst "2019-01-07T15:11:32.284-00:00",
  ...})
----
POST: takes a vector of transactions (any combination of `:put`, `:delete`, `:cas` and `:evict`) and executes them in order. This is the only "write" endpoint.

[source,bash]
----
curl -X POST \
     -H "Content-Type: application/edn" \
     -d '[[:crux.tx/put :ivan {:crux.db/id :ivan, :name "Ivan" :last-name "Petrov"}],
          [:crux.tx/put :boris {:crux.db/id :boris, :name "Boris" :last-name "Petrov"}],
          [:crux.tx/delete :maria  #inst "2012-05-07T14:57:08.462-00:00"]]' \
     $nodeURL/tx-log
----
[source,clj]
----
{:crux.tx/tx-id 7, :crux.tx/tx-time #inst "2019-01-07T16:14:19.675-00:00"}
----

== Single Node

Crux can also be run on a single node without Kafka as a pure library.
One aim is to be able to use the same library at vastly different sizes
of deployments.

TODO: Document this.
