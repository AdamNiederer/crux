= Design

== Schemaless

== Indexing

We have 4 indexes, spread across a few logical stores:

_Document Store_

* content-hash -> doc
* aid/value -> content-hash

_Transaction Log_

* eid/business-time/transact-time/tx-id -> content-hash
* content-hash -> set of eids

Querying works by looking up the attribute and value in the aid/value
index, and resolving the resulting content hash into a set of entity
ids which have at any point in time had this value. These entities are
then resolved using the bitemporal coordinates using the
eid/business-time/transact-time/tx-id index, and kept if the content
hash is the same.

Following references and joins both work by deserializing the current
node, and getting all the values for the attribute. Keywords are
resolved to their ids, and then the target content hashes are looked up
as in querying above.

Many valued attributes (cardinality many) are simply repeated in the
aid/value index and taken from the document on initial indexing, there’s
no special handling of them. Order in lists etc. are preserved by the
original document.

Range queries are served by directly scanning the aid/value index, as it
does not contain any information about time, and will be sorted by
order. The values themselves needs to be encoded in a binary format that
preserves order.

The content-hash -> doc index is potentially a LRU cache backed by a
larger, full key value store shared between the query nodes.

When looking up using transact-time, business-time defaults to
transact-time. This will filter out eventual writes done into the future
business-time, which would need to be found via an explicit
business-time/transact-time pair. The transact-time might not be an
actual date, but could be a monotonic transaction id or maybe some form
of causal context. If the transact-time is a date, it should be taken in
an consistent (monotonic) manner, and should not be assigned by the
client.

The tx-id is based on the offset of the transaction in the transaction
log. Both this tx-id, the transact-time and business-time are possible
to retrieve from the main eid/business-time/transact-time/tx-id index
without storing them inside the entity itself.

It’s worth noting that the content-hash -> doc and aid/value ->
content-hash indexes form their own key value store with secondary
indexes. The content-hash -> doc could be represented as a compacted
topic in Kafka, which could be spread on many partitions and topics (for
various types of data and retention). In this case the transactions
themselves could simply contain eid/business-time/transact-time/tx-id ->
content-hash or variants there of, pointing to the key value topic and
is kept smaller. A content hash of nil would be a retraction of the full
entity. CAS can also be implemented via supplying two content hashes.
Upserts are slightly trickier in this scenario, as this would depend on
Crux merging the documents and generate a new content hash this could
happen on the client side in conjunction with CAS, so that one knows
that one updates the expected version.

This way the values in the key value topic can be purged (evicted)
independently of rewriting the main log by overwriting them with either
nil or in a more advanced usage, a scrubbed version of the data (this
has a drawback of breaking the hash, so would need some meta data).

For full erasure, the key value store also need to keep track of the
keys a content-hash have in the aid/value index, as these need to be
purged as well, but the set can conceivably change if the indexing code
changes. This is likely better dealt with via index migrations, where
the simplest case is to simply retire nodes using an old index formats,
and hence deleting their indexes.

== Identifiers

It’s up to the users to supply their own IDs, such as UUIDs.

The advantage of this approach are:

1.  Users get to use their own (upstream) IDs, which is more sympathetic
to the enterprise reality of multiple data-stores, and for when users
are working with external data-sets that already come with IDs.
2.  External IDs are needed anyway, if data is to be sharded across
nodes and needs to be reconciled in some way.
3.  No-need for temp IDs, thus simplicity of operation.
4.  It aligns with the intuitions of Crux being an `unbundled' DB; the
ID generation management is another piece that is unbundled, given over
to the user’s control.

The downside is that external IDs will not be optimised for internal
usage, i.e being numeric IDs to be used directly as part of Crux’s
indices. Therefore IDs may will to be mapped to/from accordingly when
data goes in and out.

This could be mitigated by using MD5s rather than numerical IDs
internally (albeit at a higher cost), but a mapping will still to be
made to reconstruct the external IDs when returning data.

== Query Engine

Crux uses a Datalog dialect that’s a subset of Datomic/Datascript’s
with some extensions. The results of Crux queries can be consumed
lazily (partial results must at times be sorted in memory) and Crux is
declarative in the sense that clause order doesn’t affect the query
execution (there are some cases where this isn’t true if one relies on
the internals).

The query engine is built using the concept of ``virtual indexes'',
which bottom out to a combination of the above physical indexes on disk
or data directly in-memory. The actual queries are represented as a
composition and combination of these indexes. Things like range
constraints are applied as decorators on the lower level indexes. The
query engine itself never concerns itself with time, as this is hidden
by the lower indexes.

The query is itself ultimately represented as a single n-ary join across
variables, each potentially represented by several indexes, and combined
via an unary join across them. As the resulting tree is walked the query
engine further has a concept of constraints, which are applied to the
results as the joins between the indexes are performed. Things like
predicates and sub queries are implemented using such constraints.
Nested expressions, such as `not`, `or` and rules are executed several
times as separate sub queries on the partial results as the tree is
walked. All indexes participating in a unary join must be sorted in the
same order. All n-ary indexes (relations) participating in the parent
n-ary join must have the same variable order.

Conceptually the execution model is a combination of an n-ary worst case
optimal join and Query-Subquery (QSQ) evaluation of Datalog. The worst
case optimal join algorithm binds free variables which then are used as
arguments in QSQ. The results of the sub query are then injected an
n-ary index (relation) into the parent query, binding further variables
in the current parent query sub tree (``sideways information passing'').
Rules are evaluated via a combination of eager expansion of the rule
bodies into the parent query and QSQ recursion. `or` and `or-join` are
anonymous rules. `not` is a sub query which executes when all required
variables are bound, acting as a predicate which returns false if there
are any sub query results, filtering the parent results.

== Transactions

The four transaction (write) operations are as follows:

[source,clojure]
----
[:crux.tx/put :http://dbpedia.org/resource/Pablo_Picasso
"090622a35d4b579d2fcfebf823821298711d3867"
#inst "2018-05-18T09:20:27.966-00:00"]

[:crux.tx/cas :http://dbpedia.org/resource/Pablo_Picasso
"090622a35d4b579d2fcfebf823821298711d3867"
"048ebba27e1da223ce97dded59d46e069ddf921b"
#inst "2018-05-18T09:21:31.846-00:00"]

[:crux.tx/delete :http://dbpedia.org/resource/Pablo_Picasso
#inst "2018-05-18T09:21:52.151-00:00"]

[:crux.tx/evict :http://dbpedia.org/resource/Pablo_Picasso
#inst "2018-05-18T09:21:52.151-00:00"]
----

The business time is optional and defaults to transaction time, which is
taken from the Kafka log. Crux currently writes into the past at a
single point, so to overwrite several versions or a range in time, one
is required to submit a transaction containing several operations.
Eviction works a bit differently, and all versions at or before the
provided business time are evicted.

The hashes are the SHA-1 content hash of the documents. Crux uses an
attribute `:crux.db/id` on the documents that has to line up with the id
it is submitted under.

A document looks like this:

[source,clj]
----
{:crux.db/id :http://dbpedia.org/resource/Pablo_Picasso
 :name "Pablo"
 :last-name "Picasso"}
----

In practice when using Crux, one calls `crux.db/submit-tx` with a set of
transaction operations as above, where the hashes are replaced with
actual documents:

[source,clj]
----
[[:crux.tx/put :http://dbpedia.org/resource/Pablo_Picasso
 {:crux.db/id :http://dbpedia.org/resource/Pablo_Picasso
  :name "Pablo"
  :last-name "Picasso"}
 #inst "2018-05-18T09:20:27.966-00:00"]]
----

For each operation the id and the document are hashed, and this version
is submitted to the `tx-topic` in Kafka. The document itself is
submitted to the `doc-topic`, using its content hash as key. This latter
topic is compacted, which enables later deletion of documents.

If the transaction contains CAS operations, all CAS operations must pass
their pre-condition check or the entire transaction is aborted. This
happens at the query node during indexing, and not when submitting the
transaction.

Crux stores ``entities'', each having a stable id, and a set of EDN
documents making up its history. Apart from EDN, there’s no schema of
the documents, and no enforced concept of references. References are
simply fields where the value of an attribute is the `:crux.db/id` of
another document.

A Crux id is a type which satisfies the `crux.index.IdToBytes` protocol.
Keywords, UUIDs, URIs and SHA-1 hex strings do this out of the box. Note
that normal strings are not considered valid ids. Crux will not
automatically assigns ids. The id is always a SHA-1 hash.

The attributes will be indexed locally on each node to enable queries.
Attributes which have vectors or sets as the values will have all their
elements indexed, and as mentioned, Crux does not enforce any schema. A
document can change the type of its fields at will between versions,
though this isn’t recommended, as it leads to confusion at query time.

Indexing is done via the `crux.index.ValueToBytes` protocol. The default
is to take the SHA-1 of the value serialised by Nippy. Ids index via
`IdToBytes`. `Byte`, `Short`, `Integer`, `Long`, `Float`, `Double`,
`Date`, `Character` and `String` have implementations which respect
ordering while serialised to unsigned bytes, which is what most
underlying KV stores will use to order the keys. If the implementation
returns an empty byte array the value isn’t indexed. The value byte
arrays have a prefix tag of one byte to ensure different types don’t
overlap.

The above implies that values which are maps are simply indexed as their
hash. They can be used as a value in a query to find entities like any
other literal, but the contents of the map itself are opaque to the
index. ``Component entities'', or RDF blank nodes, must be their own
actual entities with ``anonymous'' ids and have explicit transaction
operations like any other entity.

Crux also supports a few lower-level read operations, like
`crux.doc/entities-at`, `crux.doc/entity-history` for entities from the
kv and `crux.db/get-objects` to get documents from an object store.
These internals should not be assumed to be stable APIs, but similar
functionality will be preserved.

Crux query capability is easiest summarized via an example:

[source,clj]
----
(q/q db
    '{:find  [?e2]
      :where [(follow ?e1 ?e2)]
      :args [{:?e1 :1}]
      :rules [[(follow ?e1 ?e2)
              [?e1 :follow ?e2]]
             [(follow ?e1 ?e2)
              [?e1 :follow ?t]
              (follow ?t ?e2)]]})
----

The `db` is retrieved via a call to `crux.query/db` which optionally
takes business and transaction time. The call will block until the local
index has seen the transaction time, if provided. The `crux.query/q`
takes 2 or 3 arguments, `db` and `q` but also optionally a `snapshot`
which is already opened and managed by the caller (using `with-open` for
example). This version of the call returns a lazy sequence of the
results, while the other verision provides a set. A snapshot can be
retreived from a `kv` instance via `crux.kv-store/new-snapshot`.

The `:args` key contains a relation where each map is expected to have
the same keys. These keys are turned into logic variable symbols and the
relation is joined with the rest of the query. The elements must
implement `Comparable`.

Crux does not support variables in the attribute position. The entity
position is hard coded to mean the `:crux.db/id` field.

The REST API provides the following paths: `/document`, `/history`,
`/query` for reads and `/tx-log` for writes. When using the REST API the
user doesn’t interact directly with Kafka, but calls one of the query
nodes (potentially behind a load balancer) over HTTP to interact with
Crux. As the query nodes might be at different points in the index, and
different queries might go to differnet nodes, there are currently some
read consistency issues that can arise here.

The REST API also provides an experimental end point for SPARQL 1.1
Protocol queries under `/sparql/`. Only a small subset of SPARQL is
supported, and is working by rewriting the query into the Crux datalog
dialect and there are no further RDF features by using this.

== How does Crux do it?

Crux mainly consists of two parts, the transaction and ingestion piece,
built around Kafka, and the query piece, built on top of a local KV
store such as RocksDB. The ingestion engine populates the indexes.

=== Ingestion

On the ingestion side, the main design is to split the data into two
separate topics, the `tx-topic` and the `doc-topic`. The users don’t
write directly to these topics, but use a `crux.db.TxLog` instance to do
so. Each transaction operation will be split into several messages,
where documents go into the `doc-topic` and the hashed versions of the
transaction operations go into the `tx-topic` as a single message.

The `tx-topic` is immutable, but the `doc-topic` is compacted, and keyed
by the documents content hashes, enabling eviction of the data. As data
can be purged for good using this mechanism, Crux does not lend itself
to naively be used as an event sourcing mechanism, as while the
`tx-topic` will stay intact, it might refer to documents which have
since been evicted.

The consumer side indexes both the `doc-topic` and the `tx-topic`, into
a bunch of local indexes in the KV store, which are used by the query
engine. The indexes are:

* `content-hash->doc-index` Main document store.
* `attribute+value+entity+content-hash-index` Secondary index of
attribute values, mapped to their entities and versions (content
hashes).
* `attribute+entity+value+content-hash-index` Secondary index of
attribute entities, mapped to their values and versions (content
hashes). The reverse of the above.
* `entity+bt+tt+tx-id->content-hash-index` Main temporal index, used to
find the content hash of a specific entity version.
* `meta-key->value-index` Used to store Kafka offsets and transaction
times.
