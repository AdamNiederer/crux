= Design

== Indexing

We have 4 indexes, spread across a few logical stores:

_Document Store_

* content-hash -> doc
* aid/value -> content-hash

_Transaction Log_

* eid/business-time/transact-time/tx-id -> content-hash
* content-hash -> set of eids

Querying works by looking up the attribute and value in the aid/value
index, and resolving the resulting content hash into a set of entity
ids which have at any point in time had this value. These entities are
then resolved using the bitemporal coordinates using the
eid/business-time/transact-time/tx-id index, and kept if the content
hash is the same.

Following references and joins both work by deserializing the current
node, and getting all the values for the attribute. Keywords are
resolved to their ids, and then the target content hashes are looked up
as in querying above.

Many valued attributes (cardinality many) are simply repeated in the
aid/value index and taken from the document on initial indexing, there’s
no special handling of them. Order in lists etc. are preserved by the
original document.

Range queries are served by directly scanning the aid/value index, as it
does not contain any information about time, and will be sorted by
order. The values themselves needs to be encoded in a binary format that
preserves order.

The content-hash -> doc index is potentially a LRU cache backed by a
larger, full key value store shared between the query nodes.

When looking up using transact-time, business-time defaults to
transact-time. This will filter out eventual writes done into the future
business-time, which would need to be found via an explicit
business-time/transact-time pair. The transact-time might not be an
actual date, but could be a monotonic transaction id or maybe some form
of causal context. If the transact-time is a date, it should be taken in
an consistent (monotonic) manner, and should not be assigned by the
client.

The tx-id is based on the offset of the transaction in the transaction
log. Both this tx-id, the transact-time and business-time are possible
to retrieve from the main eid/business-time/transact-time/tx-id index
without storing them inside the entity itself.

It’s worth noting that the content-hash -> doc and aid/value ->
content-hash indexes form their own key value store with secondary
indexes. The content-hash -> doc could be represented as a compacted
topic in Kafka, which could be spread on many partitions and topics (for
various types of data and retention). In this case the transactions
themselves could simply contain eid/business-time/transact-time/tx-id ->
content-hash or variants there of, pointing to the key value topic and
is kept smaller. A content hash of nil would be a retraction of the full
entity. CAS can also be implemented via supplying two content hashes.
Upserts are slightly trickier in this scenario, as this would depend on
Crux merging the documents and generate a new content hash this could
happen on the client side in conjunction with CAS, so that one knows
that one updates the expected version.

This way the values in the key value topic can be purged (evicted)
independently of rewriting the main log by overwriting them with either
nil or in a more advanced usage, a scrubbed version of the data (this
has a drawback of breaking the hash, so would need some meta data).

For full erasure, the key value store also need to keep track of the
keys a content-hash have in the aid/value index, as these need to be
purged as well, but the set can conceivably change if the indexing code
changes. This is likely better dealt with via index migrations, where
the simplest case is to simply retire nodes using an old index formats,
and hence deleting their indexes.

== Identifiers

It’s up to the users to supply their own IDs, such as UUIDs.

The advantage of this approach are:

1.  Users get to use their own (upstream) IDs, which is more sympathetic
to the enterprise reality of multiple data-stores, and for when users
are working with external data-sets that already come with IDs.
2.  External IDs are needed anyway, if data is to be sharded across
nodes and needs to be reconciled in some way.
3.  No-need for temp IDs, thus simplicity of operation.
4.  It aligns with the intuitions of Crux being an `unbundled' DB; the
ID generation management is another piece that is unbundled, given over
to the user’s control.

The downside is that external IDs will not be optimised for internal
usage, i.e being numeric IDs to be used directly as part of Crux’s
indices. Therefore IDs may will to be mapped to/from accordingly when
data goes in and out.

This could be mitigated by using MD5s rather than numerical IDs
internally (albeit at a higher cost), but a mapping will still to be
made to reconstruct the external IDs when returning data.
